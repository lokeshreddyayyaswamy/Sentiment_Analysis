# -*- coding: utf-8 -*-
"""Sentiment_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1feaUewL7ygiCyuBs9CaCYzO0s3Hwr_cY

#Load the dataset
"""

import pandas as pd
import numpy as np

df=pd.read_csv('/content/Twitter_Data.csv')
df

df['clean_text'][0]

df.dropna(inplace=True)

df.shape

df['category'].value_counts()

"""#Pre processing"""

import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

# Download necessary NLTK resources
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('wordnet')

# Initialize NLP tools
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    # Remove URLs
    text = re.sub(r"http\S+|www\S+|https\S+", "", text, flags=re.MULTILINE)

    # Remove mentions and hashtags
    text = re.sub(r"@\w+|#\w+", "", text)

    # Remove special characters and numbers
    text = re.sub(r"[^a-zA-Z\s]", "", text)

    # Convert to lowercase
    text = text.lower()

    # Tokenization
    words = word_tokenize(text)

    # Remove stopwords and apply lemmatization
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]

    # Reconstruct the cleaned sentence
    return " ".join(words)

# Apply preprocessing
df["cleaned_text"] = df["clean_text"].astype(str).apply(preprocess_text)

# Display sample
df.head()

df.drop('clean_text',axis=1,inplace=True)

df

"""#Multinomial NB"""

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# Define features and labels
X = df["cleaned_text"]  # Preprocessed text data
y = df["category"]         # Sentiment labels (should be categorical: 0, 1, 2 for neg, neu, pos)

# Convert text data to numerical representation using TF-IDF
vectorizer =TfidfVectorizer(use_idf=True,max_features=20000)  # Limit features for performance
X_tfidf = vectorizer.fit_transform(X)

X_tfidf[0]

print(X_tfidf[0])

# Split data into train (80%) and test (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

print(y_train)

# Train Naïve Bayes Classifier
nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)

# Predict on test data
y_pred = nb_model.predict(X_test)

# Evaluate performance
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.4f}")

print("\nClassification Report:\n", classification_report(y_test, y_pred))

"""#Complement NB"""

from sklearn.naive_bayes import ComplementNB

# Train Log-Count Ratio Naïve Bayes (ComplementNB)
cnb_model = ComplementNB()
cnb_model.fit(X_train, y_train)

# Predict on test data
y_pred = cnb_model.predict(X_test)

# Evaluate performance
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.4f}")

print("\nClassification Report:\n", classification_report(y_test, y_pred))

from sklearn.model_selection import GridSearchCV

# Define parameter grid
param_grid = {
    'alpha': [0.01, 0.1, 0.5, 1.0, 2.0],  # Smoothing parameter
    'norm': [True, False]  # Whether to normalize
}

# Initialize ComplementNB model
cnb_model = ComplementNB()

# Perform Grid Search with Cross Validation
grid_search = GridSearchCV(cnb_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

# Print best parameters and accuracy
print("Best Parameters:", grid_search.best_params_)
print("Best Accuracy:", grid_search.best_score_)

# Use the best model
best_cnb = grid_search.best_estimator_

y_pred = best_cnb.predict(X_test)

# Evaluate performance
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.4f}")

"""#RNN"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

# Extract text and labels
X_texts = df["cleaned_text"].astype(str).tolist()  # Ensure text is in string format
y_labels = df["category"].values  # Labels (-1, 0, 1)

# Convert labels to categorical format
y_labels = tf.keras.utils.to_categorical(y_labels, num_classes=3)  # Adjust if needed

X_texts[0]

y_labels

# Tokenization
VOCAB_SIZE = 20000  # Set vocabulary size
tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token="<OOV>")
tokenizer.fit_on_texts(X_texts)

# Convert text to sequences
X_sequences = tokenizer.texts_to_sequences(X_texts)

# Padding sequences to the same length
MAX_LEN = 40  # Set max sequence length
X_padded = pad_sequences(X_sequences, maxlen=MAX_LEN, padding='post', truncating='post')

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_padded, y_labels, test_size=0.2, random_state=42)

print(f"Vocabulary size: {len(tokenizer.word_index)}")
print(f"Example sequence: {X_sequences[0]}")

X_padded[0]

X_train[0]

X_train.shape

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Dropout
from tensorflow.keras.layers import LSTM, Bidirectional

# Define RNN model
model = Sequential([
    Embedding(input_dim=20000, output_dim=128),
    SimpleRNN(64, return_sequences=False),
    Dropout(0.3),
    Dense(16, activation="relu"),
    Dense(y_train.shape[1], activation="softmax")  # Output layer with softmax activation
])

# Compile model
model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

# Train the model
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=32)

# Evaluate on test data
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc:.4f}")

# Make predictions
y_pred = np.argmax(model.predict(X_test), axis=1)
y_true = np.argmax(y_test, axis=1)

from sklearn.metrics import classification_report
print(classification_report(y_true, y_pred))

"""#LSTM"""

# Define the LSTM model
model = Sequential([
    Embedding(input_dim=VOCAB_SIZE, output_dim=128, input_length=MAX_LEN),  # Embedding layer
    LSTM(32, return_sequences=True),  # Single-directional LSTM
    Dropout(0.3),  # Regularization
    LSTM(16),  # Single-directional LSTM
    Dropout(0.3),
    Dense(16, activation="relu"),
    Dense(3, activation="softmax")  # 3 output classes (-1, 0, 1)
])

# Compile the model
model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

# Train the model
history_lstm = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test), verbose=1)

# Evaluate on test data
test_loss_lstm, test_acc_lstm = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc_lstm:.4f}")

# Make predictions
y_pred_lstm = np.argmax(model.predict(X_test), axis=1)
y_true_lstm = np.argmax(y_test, axis=1)

from sklearn.metrics import classification_report
print(classification_report(y_true_lstm, y_pred_lstm))

"""#Bi-LSTM"""

# Define the LSTM model
model = Sequential([
    Embedding(input_dim=VOCAB_SIZE, output_dim=128, input_length=MAX_LEN),  # Embedding layer
    Bidirectional(LSTM(32, return_sequences=True)),  # BiLSTM for better understanding
    Dropout(0.3),  # Regularization
    Bidirectional(LSTM(16)),
    Dropout(0.3),
    Dense(16, activation="relu"),
    Dense(3, activation="softmax")  # 3 output classes (-1, 0, 1)
])

# Compile the model
model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

# Train the model
history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test), verbose=1)

# Evaluate on test data
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc:.4f}")

# Make predictions
y_pred = np.argmax(model.predict(X_test), axis=1)
y_true = np.argmax(y_test, axis=1)

from sklearn.metrics import classification_report
print(classification_report(y_true, y_pred))

